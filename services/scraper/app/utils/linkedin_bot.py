import os
import asyncio
import shutil
import re
from datetime import datetime, timezone, timedelta
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError


class LinkedInBot:
    def __init__(self, email=None, password=None, headless=True, debug=False, slow_mo=None):
        self.email = email
        self.password = password
        self.headless = headless
        self.debug = debug
        self.slow_mo = slow_mo
        self.playwright = None
        self.browser = None
        self.context = None
        self.page = None

    async def start(self, state_file=None):
        """Start Playwright and browser with persistent session"""
        self.playwright = await async_playwright().start()
        
        # Set default state file path relative to the bot's location
        if state_file is None:
            # Get the directory where this file is located
            current_dir = os.path.dirname(os.path.abspath(__file__))
            # Go up two levels: from app/utils/ to services/scraper/
            scraper_dir = os.path.dirname(os.path.dirname(current_dir))
            self.state_file = os.path.join(scraper_dir, "state.json")
        else:
            self.state_file = state_file

        launch_options = {
            "headless": self.headless,
            "args": [
                "--no-sandbox",
                "--disable-dev-shm-usage",
                "--disable-blink-features=AutomationControlled",
                "--disable-web-security",
                "--disable-features=VizDisplayCompositor",
            ],
            "timeout": 60000
        }
        
        if self.slow_mo:
            launch_options["slow_mo"] = self.slow_mo
        if not self.headless and self.debug:
            launch_options["devtools"] = True

        try:
            self.browser = await self.playwright.chromium.launch(**launch_options)
        except Exception as e:
            print(f"‚ö†Ô∏è Chromium launch failed: {e}")
            chrome_path = self._find_chrome_executable()
            if chrome_path:
                launch_options["executable_path"] = chrome_path
                self.browser = await self.playwright.chromium.launch(**launch_options)
            else:
                raise

        # Check if state file exists and load it
        context_options = {
            "viewport": {"width": 1920, "height": 1080},
            "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                         "AppleWebKit/537.36 (KHTML, like Gecko) "
                         "Chrome/120.0.0.0 Safari/537.36",
            "locale": "en-US"
        }
        
        # Load existing session state if available
        if os.path.exists(self.state_file):
            try:
                context_options["storage_state"] = self.state_file
                print(f"üìÇ Loading existing session from {self.state_file}")
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to load session state: {e}")
                print("üî• Starting fresh session...")
        else:
            print(f"üîç No existing session found at {self.state_file}, starting fresh...")

        self.context = await self.browser.new_context(**context_options)
        
        # Block unnecessary resources to speed up loading
        await self.context.route("**/*.{woff,woff2}", lambda route: route.abort())
        
        self.page = await self.context.new_page()
        self.page.set_default_timeout(60000)

        if self.debug:
            self.page.on("console", lambda msg: print(f"üñ• Console: {msg.text}"))
            self.page.on("response", lambda response: print(f"üì° Response: {response.status} {response.url}"))

        print("‚úÖ Browser started")

    async def login(self, manual_wait=False, force_login=False):
        """Login into LinkedIn with session persistence"""
        try:
            # First, check if we're already logged in by checking for LinkedIn main page elements
            if not force_login:
                print("üîç Checking if already logged in...")
                await self.page.goto("https://www.linkedin.com/feed", wait_until="domcontentloaded")
                await asyncio.sleep(3)
                
                # Check if we can find the search box (indicates we're logged in)
                search_box = await self.page.query_selector("input[placeholder*='Search']")
                if search_box:
                    print("‚úÖ Already logged in using saved session!")
                    return True
                else:
                    print("üî• Session expired or invalid, proceeding with fresh login...")
            
            print("üîç Navigating to LinkedIn login...")
            await self.page.goto("https://www.linkedin.com/login", wait_until="domcontentloaded")
            
            # Wait for login form
            await self.page.wait_for_selector("input#username", timeout=10000)
            
            # Fill credentials
            await self.page.fill("input#username", self.email)
            await asyncio.sleep(1)
            await self.page.fill("input#password", self.password)
            await asyncio.sleep(1)
            
            # Click login button
            await self.page.click("button[type='submit']")
            print("üîç Login submitted...")
            
            # Wait for successful login or CAPTCHA/2FA
            try:
                # Try to wait for the main LinkedIn page
                await self.page.wait_for_selector("input[placeholder*='Search']", timeout=15000)
                print("‚úÖ Login successful")
                
                # Save session state after successful login
                await self._save_session_state()
                return True
                
            except PlaywrightTimeoutError:
                # Check for CAPTCHA or verification
                captcha_present = await self.page.query_selector("iframe[title*='reCAPTCHA']")
                verification_present = await self.page.query_selector("input[name='pin']")
                
                if captcha_present or verification_present or manual_wait:
                    print("‚ö†Ô∏è CAPTCHA/2FA detected or manual wait requested...")
                    print("üö® MANUAL ACTION REQUIRED:")
                    print("   1. Complete 2FA/CAPTCHA verification in the browser")
                    print("   2. Wait for redirect to LinkedIn feed")
                    print("   3. Session will be automatically saved")
                    
                    if manual_wait:
                        await asyncio.sleep(30)
                    else:
                        # Wait for user to complete verification (extended timeout)
                        print("‚è≥ Waiting up to 5 minutes for manual verification...")
                        for i in range(300):  # Wait up to 5 minutes
                            try:
                                await self.page.wait_for_selector("input[placeholder*='Search']", timeout=1000)
                                print("‚úÖ Login successful after verification")
                                
                                # Save session state after successful manual login
                                await self._save_session_state()
                                return True
                            except PlaywrightTimeoutError:
                                if i % 30 == 0:  # Print progress every 30 seconds
                                    remaining = (300 - i) // 60
                                    print(f"‚è≥ Still waiting... ({remaining} minutes remaining)")
                                continue
                    
                    # Final check
                    search_box = await self.page.query_selector("input[placeholder*='Search']")
                    if search_box:
                        print("‚úÖ Login successful")
                        await self._save_session_state()
                        return True
                    else:
                        print("‚ùå Login verification timeout. Please run in non-headless mode for manual completion.")
                        print("üí° Tip: Run with headless=False to complete 2FA manually, then switch back to headless=True")
                        raise Exception("Login failed - verification timeout")
                else:
                    raise Exception("Login failed - unknown error")
                    
        except Exception as e:
            print(f"‚ùå Login error: {e}")
            raise

    async def _save_session_state(self):
        """Save current session state to file"""
        try:
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(self.state_file), exist_ok=True)
            
            # Save the storage state (cookies, localStorage, etc.)
            await self.context.storage_state(path=self.state_file)
            print(f"üíæ Session state saved to {self.state_file}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to save session state: {e}")

    async def ensure_logged_in(self):
        """Utility method to ensure user is logged in before scraping"""
        try:
            # Quick check if we're on a LinkedIn page and logged in
            current_url = self.page.url
            if "linkedin.com" not in current_url:
                await self.page.goto("https://www.linkedin.com/feed", wait_until="domcontentloaded")
            
            # Check for search box
            search_box = await self.page.query_selector("input[placeholder*='Search']")
            if not search_box:
                print("üî• Session expired, attempting re-login...")
                return await self.login(force_login=True)
            
            return True
        except Exception as e:
            print(f"‚ö†Ô∏è Login check failed: {e}")
            return False

    async def scrape_user_posts(self, profile_url, n_posts=5):
        """Scrape posts from LinkedIn profile with enhanced navigation and fallback strategies"""
        try:
            # Ensure we're logged in before scraping
            if not await self.ensure_logged_in():
                raise Exception("Failed to ensure login status")
            
            print(f"üîç Navigating to profile: {profile_url}")
            await self.page.goto(profile_url, wait_until="domcontentloaded")
            
            # Wait for page to load
            await asyncio.sleep(4)
            
            posts_data = []
            posts_tab_success = False
            
            # Strategy 1: Try to navigate to dedicated posts page
            print("üîç Attempting to navigate to dedicated posts page...")
            try:
                # Multiple strategies to get to posts page
                posts_page_urls = [
                    f"{profile_url.rstrip('/')}/recent-activity/all/",
                    f"{profile_url.rstrip('/')}/recent-activity/posts/",
                    f"{profile_url.rstrip('/')}/recent-activity/"
                ]
                
                for posts_url in posts_page_urls:
                    try:
                        print(f"üîç Trying direct navigation to: {posts_url}")
                        await self.page.goto(posts_url, wait_until="domcontentloaded")
                        await asyncio.sleep(4)
                        
                        # Check if we successfully landed on posts page
                        current_url = self.page.url
                        if "recent-activity" in current_url:
                            print("‚úÖ Successfully navigated to posts page via direct URL")
                            posts_tab_success = True
                            break
                            
                    except Exception as e:
                        if self.debug:
                            print(f"‚ö†Ô∏è Direct navigation failed for {posts_url}: {e}")
                        continue
                
                # If direct navigation failed, try clicking the button
                if not posts_tab_success:
                    await self.page.goto(profile_url, wait_until="domcontentloaded")
                    await asyncio.sleep(4)
                    
                    posts_tab_success = await self._try_posts_button_click()
                
            except Exception as e:
                print(f"‚ö†Ô∏è Posts page navigation failed: {e}")
            
            # Strategy 2: Scrape from current page (either posts page or main profile)
            if posts_tab_success:
                print("üìú Scrolling to load posts from dedicated posts page...")
                # More aggressive scrolling for posts page
                await self._scroll_and_load_posts_page(n_posts)
            else:
                print("üìú Scrolling to load posts from main profile page...")
                await self._scroll_and_load(n_posts)
            
            # Extract posts
            posts_data = await self._extract_all_posts(n_posts, profile_url)
            
            print(f"‚úÖ Scraped {len(posts_data)} posts from {profile_url}")
            return posts_data
            
        except Exception as e:
            print(f"‚ùå Error scraping profile posts: {e}")
            return []

    async def _try_posts_button_click(self):
        """Try to click the 'Show all posts' button with multiple strategies"""
        try:
            print("üîç Looking for 'Show all posts' button...")
            
            post_tab_selectors = [
                "footer a.profile-creator-shared-content-view__footer-action:has-text('Show all posts')",
                "a.profile-creator-shared-content-view__footer-action:has-text('Show all posts')",
                "a.profile-creator-shared-content-view__footer-action.artdeco-button",
                "a[href*='/recent-activity/all/']",
                "a[href*='/recent-activity/posts/']",
                "a[href*='/recent-activity/']",
                "a:has-text('Show all posts')",
                "button:has-text('Show all posts')"
            ]
            
            for selector in post_tab_selectors:
                try:
                    posts_tab = await self.page.wait_for_selector(selector, timeout=5000)
                    if posts_tab:
                        button_text = await posts_tab.inner_text()
                        print(f"‚úÖ Found element with text: '{button_text.strip()}'")
                        
                        if "show all posts" in button_text.lower() or "recent-activity" in await posts_tab.get_attribute('href') or '':
                            print(f"‚úÖ Found posts navigation element with selector: {selector}")
                            
                            await posts_tab.scroll_into_view_if_needed()
                            await asyncio.sleep(1)
                            
                            # Try different click strategies
                            try:
                                await posts_tab.click()
                            except:
                                # Fallback: JavaScript click
                                await self.page.evaluate("(element) => element.click()", posts_tab)
                            
                            await asyncio.sleep(5)
                            
                            current_url = self.page.url
                            if "recent-activity" in current_url:
                                print("‚úÖ Successfully navigated to posts page")
                                return True
                            else:
                                print("‚ö†Ô∏è Click registered but URL didn't change to posts page")
                                
                except PlaywrightTimeoutError:
                    continue
                except Exception as e:
                    if self.debug:
                        print(f"‚ö†Ô∏è Error with selector {selector}: {e}")
                    continue
            
            return False
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error trying posts button click: {e}")
            return False

    async def _scroll_and_load_posts_page(self, n_posts):
      """Optimized scrolling for fewer posts"""
      posts_loaded = 0
      scroll_count = 0
      max_scrolls = min(8, n_posts // 2 + 3)  # Dynamic max based on needed posts
      no_new_posts_count = 0
    
      while posts_loaded < n_posts and scroll_count < max_scrolls and no_new_posts_count < 3:
        # Scroll to bottom
        await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        await asyncio.sleep(2)  # Reduced from 3 to 2 seconds
        
        # Skip "Show more" clicking for efficiency if we have enough posts
        if posts_loaded < n_posts:
            try:
                show_more_buttons = await self.page.query_selector_all("button:has-text('Show more')")
                for button in show_more_buttons[:2]:  # Limit to first 2 buttons
                    try:
                        await button.click()
                        await asyncio.sleep(1)  # Reduced wait time
                    except:
                        pass
            except:
                pass
        
        # Count posts (same logic)
        previous_posts_loaded = posts_loaded
        post_selectors = [
            "div[data-urn*='activity']",
            "div.feed-shared-update-v2", 
            "article.artdeco-card",
            "div.profile-creator-shared-feed-update__container"
        ]
        
        for selector in post_selectors:
            try:
                posts = await self.page.query_selector_all(selector)
                posts_loaded = len(posts)
                if posts_loaded >= n_posts:  # Early exit when we have enough
                    print(f"‚úÖ Target reached: {posts_loaded}/{n_posts} posts")
                    return posts_loaded
                if posts_loaded > 0:
                    break
            except:
                continue
        
        print(f"üìä Posts loaded: {posts_loaded}/{n_posts}")
        
        # Check progress (reduced tolerance)
        if posts_loaded == previous_posts_loaded:
            no_new_posts_count += 1
        else:
            no_new_posts_count = 0
        
        scroll_count += 1
    
      print(f"‚úÖ Finished scrolling. Found {posts_loaded} posts")
      return posts_loaded

    async def _extract_all_posts(self, n_posts, profile_url):
        """Extract posts with enhanced selectors and better error handling"""
        posts_data = []
        
        post_selectors = [
            "div[data-urn*='activity']",
            "div.feed-shared-update-v2",
            "article.artdeco-card",
            "div.profile-creator-shared-feed-update__container"
        ]
        
        posts = []
        for selector in post_selectors:
            try:
                posts = await self.page.query_selector_all(selector)
                if posts:
                    print(f"‚úÖ Found {len(posts)} posts with selector: {selector}")
                    break
            except:
                continue
        
        if not posts:
            print("‚ùå No posts found with any selector")
            return []
        
        # Process posts
        processed_count = 0
        for i, post in enumerate(posts):
            if processed_count >= n_posts:
                break
            
            print(f"üîç Processing post {processed_count + 1}/{n_posts}")
            
            try:
                # Extract post content
                post_text = await self._extract_post_content(post)
                if not post_text or len(post_text.strip()) < 10:
                    print(f"  ‚ö†Ô∏è Skipping post {i+1}: insufficient content")
                    continue
                
                # Extract engagement metrics
                metrics = await self._extract_engagement_metrics_improved(post)
                
                
                # Create unique identifier for deduplication
                post_id = f"{post_text[:50]}_{metrics.get('likes', 0)}_{metrics.get('comments', 0)}"
                
                # Check for duplicates
                if any(existing_post.get('id') == post_id for existing_post in posts_data):
                    print(f"  ‚ö†Ô∏è Skipping post {i+1}: duplicate content")
                    continue
                
                post_data = {
                    "id": post_id,
                    "text": post_text,
                    "likes": metrics.get("likes", 0),
                    "comments": metrics.get("comments", 0),
                    "reposts": metrics.get("reposts", 0),
                    "engagement": metrics.get("engagement", 0),
                    "scraped_at": datetime.now(timezone(timedelta(hours=5, minutes=30))).isoformat(),
                    "source": "profile",
                    "profile_url": profile_url
                }
                
                posts_data.append(post_data)
                processed_count += 1
                
                print(f"  ‚úÖ Post {processed_count}: Likes={metrics['likes']}, Comments={metrics['comments']}, Reposts={metrics['reposts']}")
                
                if self.debug:
                    print(f"üîç Post text: {post_text[:100]}...")
                    
            except Exception as e:
                if self.debug:
                    print(f"‚ö†Ô∏è Error processing post {i}: {e}")
                continue
        
        return posts_data

    async def scrape_hashtag_posts(self, hashtag, n_posts=5):
      """Scrape posts for a specific hashtag"""
      try:
        # Ensure we're logged in before scraping
        if not await self.ensure_logged_in():
            raise Exception("Failed to ensure login status")
            
        # Clean hashtag
        clean_hashtag = hashtag.lstrip('#')
        search_url = f"https://www.linkedin.com/search/results/content/?keywords=%23{clean_hashtag}"
        
        print(f"üè∑Ô∏è Navigating to hashtag via search: #{clean_hashtag}")
        await self.page.goto(search_url, wait_until="domcontentloaded")
        await asyncio.sleep(5)  # can reduce to 3 if speed is more important

        print("üìú Scrolling to load hashtag posts...")
        posts_found = await self._scroll_and_load(n_posts)
        
        if posts_found == 0:
            print("‚ö†Ô∏è No hashtag posts found.")
            return []

        return await self._scrape_posts_generic(n_posts, {"source": "hashtag", "hashtag": clean_hashtag})
                    
      except Exception as e:
        print(f"‚ùå Error scraping hashtag posts: {e}")
        return []


    async def _scroll_and_load(self, n_posts):
        """Scroll until at least n_posts loaded"""
        posts_loaded = 0
        last_height = await self.page.evaluate("document.body.scrollHeight")
        max_scrolls = 20
        scroll_count = 0
        
        while posts_loaded < n_posts and scroll_count < max_scrolls:
            await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(3)
            
            post_selectors = [
                "div[data-urn*='activity']",
                "div.feed-shared-update-v2", 
                "article.artdeco-card"
            ]
            
            for selector in post_selectors:
                try:
                    posts = await self.page.query_selector_all(selector)
                    posts_loaded = len(posts)
                    if posts_loaded > 0:
                        break
                except:
                    continue
            
            print(f"üìä Posts loaded: {posts_loaded}/{n_posts}")
            
            new_height = await self.page.evaluate("document.body.scrollHeight")
            if new_height == last_height:
                scroll_count += 1
                if scroll_count >= 3:
                    break
            else:
                scroll_count = 0
            last_height = new_height
        
        print(f"‚úÖ Finished scrolling. Found {posts_loaded} posts")
        return posts_loaded

    async def _extract_engagement_metrics_improved(self, post_element):
        """Extract likes, comments, reposts using the same logic as Selenium version"""
        metrics = {"likes": 0, "comments": 0, "reposts": 0, "engagement": 0}
        
        try:
            # Extract likes/reactions
            like_selectors = [
                "span.social-details-social-counts__reactions-count",
                "button[data-reaction-details] span.social-details-social-counts__reactions-count",
                "button[aria-label*='reactions'] span.social-details-social-counts__reactions-count"
            ]
            
            for like_selector in like_selectors:
                try:
                    likes_element = await post_element.query_selector(like_selector)
                    if likes_element:
                        likes_text = await likes_element.inner_text()
                        if likes_text:
                            metrics["likes"] = self._extract_int(likes_text.strip())
                            if self.debug:
                                print(f"  Likes found: {likes_text} -> {metrics['likes']}")
                            break
                except:
                    continue
            
            # Extract comments
            try:
                comments_span = await post_element.query_selector("li.social-details-social-counts__comments span[aria-hidden='true']")
                if comments_span:
                    comments_text = await comments_span.inner_text()
                    if self.debug:
                        print(f"  Comments span text found: '{comments_text}'")
                    
                    if comments_text and 'comment' in comments_text.lower():
                        metrics["comments"] = self._extract_int(comments_text.strip())
                        if self.debug:
                            print(f"  Comments final result: {metrics['comments']}")
                else:
                    raise Exception("Comments span not found")
                    
            except Exception as e:
                if self.debug:
                    print(f"  Comments extraction failed: {str(e)}")
                # Fallback to button aria-label
                try:
                    comments_button = await post_element.query_selector("li.social-details-social-counts__comments button")
                    if comments_button:
                        aria_label = await comments_button.get_attribute('aria-label')
                        if aria_label and 'comment' in aria_label.lower():
                            metrics["comments"] = self._extract_int(aria_label)
                            if self.debug:
                                print(f"  Comments from aria-label: {metrics['comments']}")
                except:
                    pass
            
            # Extract reposts/shares
            try:
                reposts_button = await post_element.query_selector("button[aria-label*='reposts of']")
                if reposts_button:
                    reposts_span = await reposts_button.query_selector("span[aria-hidden='true']")
                    if reposts_span:
                        reposts_text = await reposts_span.inner_text()
                        if self.debug:
                            print(f"  Reposts span text found: '{reposts_text}'")
                        
                        if reposts_text and 'repost' in reposts_text.lower():
                            metrics["reposts"] = self._extract_int(reposts_text.strip())
                            if self.debug:
                                print(f"  Reposts final result: {metrics['reposts']}")
                else:
                    raise Exception("Reposts button not found")
                    
            except Exception as e:
                if self.debug:
                    print(f"  Reposts extraction failed: {str(e)}")
                # Fallback to button aria-label
                try:
                    reposts_button = await post_element.query_selector("button[aria-label*='reposts of']")
                    if reposts_button:
                        aria_label = await reposts_button.get_attribute('aria-label')
                        if aria_label and 'repost' in aria_label.lower():
                            metrics["reposts"] = self._extract_int(aria_label)
                            if self.debug:
                                print(f"  Reposts from aria-label: {metrics['reposts']}")
                except:
                    if self.debug:
                        print("  Reposts: Not found or 0")
                    pass
            
            # Calculate total engagement
            metrics["engagement"] = metrics["likes"] + metrics["comments"] + metrics["reposts"]
            
        except Exception as e:
            if self.debug:
                print(f"‚ö†Ô∏è Error extracting engagement metrics: {e}")
        
        return metrics

    async def _scrape_posts_generic(self, n_posts, meta):
        """Generic post scraper used by both user and hashtag methods"""
        posts = []
        post_selectors = [
            "div[data-urn*='activity']",
            "div.feed-shared-update-v2",
            "article.artdeco-card"
        ]
        
        post_elements = []
        for selector in post_selectors:
            try:
                post_elements = await self.page.query_selector_all(selector)
                if post_elements:
                    print(f"‚úÖ Found {len(post_elements)} posts using selector: {selector}")
                    break
            except:
                continue
        
        if not post_elements:
            print("‚ùå No posts found with any selector")
            return posts

        for i, post in enumerate(post_elements[:n_posts]):
            print(f"üîç Scraping post {i+1}/{min(n_posts, len(post_elements))}")
            
            try:
                # Extract post text
                text = await self._extract_post_content(post)
                if not text or len(text.strip()) < 10:
                    continue

                # Extract engagement metrics
                metrics = await self._extract_engagement_metrics_improved(post)

                post_data = {
                    "text": text,
                    "likes": metrics["likes"],
                    "comments": metrics["comments"], 
                    "reposts": metrics["reposts"],
                    "engagement": metrics["engagement"],
                    "scraped_at": datetime.now().isoformat()
                }
                post_data.update(meta)
                posts.append(post_data)
                
                print(f"  üìä Post {i+1}: Likes={metrics['likes']}, Comments={metrics['comments']}, Reposts={metrics['reposts']}")

            except Exception as e:
                if self.debug:
                    print(f"‚ö†Ô∏è Error processing post {i}: {e}")
                continue

        print(f"‚úÖ Successfully scraped {len(posts)} posts")
        return posts

    async def _extract_post_content(self, post_element):
        """Extract post text content with multiple fallback strategies"""
        try:
            content_selectors = [
                "div.update-components-text",
                "div.feed-shared-text",
                "div.break-words",
                "span[dir='ltr']"
            ]
            
            for selector in content_selectors:
                try:
                    content_element = await post_element.query_selector(selector)
                    if content_element:
                        text = await content_element.inner_text()
                        if text and len(text.strip()) > 10:
                            # Clean the text
                            text = re.sub(r'\s+', ' ', text.strip())
                            text = re.sub(r'see more$', '', text, flags=re.IGNORECASE)
                except:
                    continue
            
            # Fallback: get all text from post
            all_text = await post_element.inner_text()
            if all_text:
                # Remove common LinkedIn UI text
                cleaned_text = re.sub(r'(Like|Comment|Repost|Send|Share|\d+\s*(likes?|comments?|reposts?))', '', all_text)
                cleaned_text = re.sub(r'\s+', ' ', cleaned_text.strip())
                if len(cleaned_text) > 10:
                    return cleaned_text[:500]  # Limit length
            
            return ""
            
        except Exception as e:
            if self.debug:
                print(f"‚ö†Ô∏è Error extracting post content: {e}")
            return ""

    def _extract_int(self, text):
        """Convert '1,234' / '1.2K' / '6 comments' like strings to int"""
        if not text:
            return 0
        
        if self.debug:
            print(f"    _extract_int input: '{text}'")
        
        # First, try to find numbers directly with regex
        match = re.findall(r"\d+", text)
        if self.debug:
            print(f"    _extract_int regex matches: {match}")
        
        if match:
            result = int(match[0])
            if self.debug:
                print(f"    _extract_int final result: {result}")
            return result
        
        # If no numbers found, try the K/M conversion
        text_clean = text.lower().replace(",", "").replace(" ", "")
        if self.debug:
            print(f"    _extract_int cleaned: '{text_clean}'")
        
        if "k" in text_clean:
            try:
                result = int(float(text_clean.replace("k", "")) * 1000)
                if self.debug:
                    print(f"    _extract_int K result: {result}")
                return result
            except:
                return 0
        if "m" in text_clean:
            try:
                result = int(float(text_clean.replace("m", "")) * 1000000)
                if self.debug:
                    print(f"    _extract_int M result: {result}")
                return result
            except:
                return 0

        if self.debug:
            print(f"    _extract_int no matches found")
        return 0

    def _find_chrome_executable(self):
        """Find Chrome path if Chromium fails"""
        possible_paths = [
            r"C:\Program Files\Google\Chrome\Application\chrome.exe",
            r"C:\Program Files (x86)\Google\Chrome\Application\chrome.exe",
            os.path.expanduser(r"~\AppData\Local\Google\Chrome\Application\chrome.exe"),
            "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome",
            "/usr/bin/google-chrome",
            "/usr/bin/google-chrome-stable",
            "/usr/bin/chromium-browser",
            "/snap/bin/chromium",
        ]
        for path in possible_paths:
            if os.path.exists(path):
                return path
        return shutil.which("chrome") or shutil.which("google-chrome") or shutil.which("chromium")

    async def close(self, keep_open=False):
        """Close browser with proper cleanup order and error handling"""
        if keep_open:
         print("üîç Debug mode: keeping browser open")
         return
    
        errors = []

        try:
        # Close in reverse order (page ‚Üí context ‚Üí browser ‚Üí playwright)
         if getattr(self, "page", None):
            try:
                await self.page.close()
                self.page = None
            except Exception as e:
                errors.append(f"Page close error: {e}")
 
         if getattr(self, "context", None):
            try:
                await self.context.close()
                self.context = None
            except Exception as e:
                errors.append(f"Context close error: {e}")

         if getattr(self, "browser", None):
            try:
                await self.browser.close()
                self.browser = None
            except Exception as e:
                errors.append(f"Browser close error: {e}")

         if getattr(self, "playwright", None):
            try:
                await self.playwright.stop()
                self.playwright = None
            except Exception as e:
                errors.append(f"Playwright stop error: {e}")

         if errors:
            print(f"‚ö†Ô∏è Browser close completed with errors: {'; '.join(errors)}")
         else:
            print("‚úÖ Browser closed cleanly")

        except Exception as e:
          print(f"‚ùå Critical error during browser close: {e}")
          # Fallback: force cleanup
          self.page = None
          self.context = None
          self.browser = None
          self.playwright = None
          
    async def is_fully_closed(self):
        """Verify all resources are properly closed"""
        return all([
            self.page is None,
            self.context is None, 
            self.browser is None,
            self.playwright is None
        ])